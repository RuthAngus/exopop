%
%  RULES OF THE GAME
%
%  * 80 characters
%  * line breaks at the ends of sentences
%  * eqnarrys ONLY
%  * ALWAYS cite inside parens () and use \citealt{}; no in-line cites
%  * that is all.
%

\documentclass[12pt,preprint]{aastex}

\include{vc}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{subfigure}

\newcommand{\project}[1]{{\sffamily #1}}
\newcommand{\emcee}{\project{emcee}}
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\license}{MIT License}

\newcommand{\paper}{\textsl{Article}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\bvec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2} \emph{#1} TODO: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\hoggtodo}[1]{\todo{HOGG}{blue}{#1}}
\newcommand{\mortontodo}[1]{\todo{MORTON}{green}{#1}}

% Document specific variables.
\newcommand{\rate}{\ensuremath{\Gamma}}
\newcommand{\ratepar}{{\ensuremath{\theta}}}
\newcommand{\ratepars}{{\ensuremath{\bvec{\ratepar}}}}
\newcommand{\obs}[1]{\ensuremath{\hat{#1}}}
\newcommand{\radius}{\ensuremath{R}}
\newcommand{\period}{\ensuremath{P}}
\newcommand{\completeness}{{\ensuremath{Q_\mathrm{c}}}}
\newcommand{\transitprob}{{\ensuremath{Q_\mathrm{t}}}}
\newcommand{\data}{{\ensuremath{\bvec{x}}}}
\newcommand{\entry}{{\ensuremath{\bvec{w}}}}
\newcommand{\catalog}{{\ensuremath{\bvec{\entry}}}}

\newcommand{\interim}{{\ensuremath{\bvec{\alpha}}}}

\newcommand{\binarea}{{\ensuremath{\Delta}}}
\newcommand{\bincenter}{{\ensuremath{\bvec{x}}}}
\newcommand{\binheight}{{\ensuremath{w}}}
\newcommand{\binheights}{{\ensuremath{\bvec{\binheight}}}}

\newcommand{\mean}{{\ensuremath{\mu}}}
\newcommand{\smooth}{{\ensuremath{\lambda}}}
\newcommand{\smoothpars}{{\ensuremath{\bvec{\smooth}}}}
\newcommand{\cov}{{\ensuremath{\mathrm{K}}}}

\newcommand{\modela}{\emph{Catalog A}}
\newcommand{\modelb}{\emph{Catalog B}}

\newcommand{\gammaearth}{{\ensuremath{\rate_\oplus}}}

\begin{document}

\title{%
  Inferring the distribution and rate of Earth-like exoplanets \\
  from noisy individual-planet inferences
}

\newcommand{\nyu}{2}
\newcommand{\mpia}{3}
\newcommand{\princeton}{4}
\newcommand{\berkeley}{5}
\author{%
    Daniel~Foreman-Mackey\altaffilmark{1,\nyu},
    David~W.~Hogg\altaffilmark{\nyu,\mpia},
    Timothy~D.~Morton\altaffilmark{\princeton},
    \etal
}
\altaffiltext{1}         {To whom correspondence should be addressed:
                          \url{danfm@nyu.edu}}
\altaffiltext{\nyu}      {Center for Cosmology and Particle Physics,
                          Department of Physics, New York University,
                          4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{\mpia}     {Max-Planck-Institut f\"ur Astronomie,
                          K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{\princeton}{Department of Astrophysics, Princeton University,
                          Princeton, NJ, 08540, USA} % CHECK THIS ZIP CODE

\begin{abstract}

An impressive number of exoplanets have been discovered and it is of great
scientific interest to study what we can say about the exoplanet occurrence
rate function given these discoveries.
Every catalog is censored by non-trivial selection effects and detection
efficiencies, and the measurements of the system properties (period, radius,
\etc) are noisy.
We present a general hierarchical probabilistic framework for making justified
inferences about the occurrence rate of exoplanets taking into account survey
completeness \emph{and observational uncertainties}.
By applying our method to synthetic catalogs, we demonstrate that it produces
more accurate estimates of the underlying occurrence rate than the standard
inverse-detection-efficiency weighting procedure.
When applied to an existing catalog of small planet candidates around G dwarfs
(Petigura \etal\ 2013), our method confirms previous measurements of a
flattening radius distribution near Earth's radius.
Although no true extra-solar Earth analog is known, hundreds of planets have
been found around Sun-like stars that are either Earth-sized but on shorter
periods, or else on year-long orbits but somewhat larger.
Under strong assumptions, it has been shown that these populations permit an
uncertain extrapolated estimate of the probability or rate at which Sun-like
stars host true Earth analogs.
We relax these assumptions by modeling the occurrence rate as a
multidimensional Gaussian process and find that the rate density of Earth
analogs is about two percent (per natural logarithmic bin in period and
radius) at low signal-to-noise.

\end{abstract}

\keywords{%
exoplanets: sickness
---
exoplanets: eta-Earth
---
code: open-source
---
keywords: made-up-by-Hogg
}

\section{Introduction}

% CITATIONS:
% \begin{itemize}
% \item
% \citet{fressin-fp} --- simultaneous occurrence rate and false positive rate
% \item
% \citet{morton} --- false positive rate
% \item
% \citet{morton-swift} --- KDE estimator for radius distribution of cool KOIs
% \item
% \citet{howard} --- inv-det-eff for short-period candidates around Sun-like
% stars
% \item
% \citet{dressing} --- inv-det-eff for cool KOIs
% \item
% \citet{dong} --- semi-analytic completeness, likelihood function-ish,
% turn-over at small radii, non-separability
% \item
% \citet{tabachnik} ---- Poisson likelihood
% \item
% \citet{tremaine} --- statistics of multis
% \item
% \citet{youdin} --- Poisson likelihood
% \item
% \citet{catanzarite} --- extrapolation to Eta-Earth assuming complete
% \item
% \citet{traub} --- extrapolation to Eta-Earth assuming complete
% \end{itemize}

NASA's \kepler\ mission has enabled the discovery of thousands of exoplanet
candidates (\dfmtodo{CITE KEPLER}).
Many of these planets orbit Sun-like stars (\citealt{petigura}), where the
definition of Sun-like is given in terms of the star's temperature and surface
gravity.
Given these catalogs, it is interesting to ask what we can say about the
occurrence rate of exoplanets as a function of their physical parameters
(period, radius, \etc).
The two main reasons why this is interesting are that \emph{(a)} theories of
planet formation make predictions about the shape of this function
(\dfmtodo{CITE}), and \emph{(b)} it allows us to place probabilistic
constraints on the rate of Earth analogs\footnote{We define, in detail, what
we mean by Earth analog below but, in short, we mean Earth-sized planets on
year-long orbits around Sun-like stars.}.

\citet{petigura} recently published constraints on this function based on
their independent analysis of the \kepler\ light curves for 42,000 Sun-like
stars but the study of exoplanet occurrence rates has a long history.
\dfmtodo{Summarize history with lotsa citations.}

A very important component of any study of exoplanet occurrence rates is the
treatment of detection efficiency.
Qualitatively, in a transit survey, small planets with long periods are much
harder to detect than large planets orbiting close to their star.
This effect is degenerate with any inferences about the rate function and it
can be hard to constraint quantitatively.
In practice, there are three methods for taking this effect into account:
\emph{(a)} making conservative cuts on the candidates and assuming that the
resulting catalog is complete (\citealt{catanzarite, traub}; \dfmtodo{CITE}),
\emph{(b)} asserting an analytic form for the detection efficiency as a
function of approximate signal-to-noise (\citealt{howard, dressing, dong,
fressin-fp, morton-swift}; \dfmtodo{CITE}), and \emph{(c)} determining the
detection efficiency empirically by injecting synthetic signals into the raw
data and testing recovery rates (\citealt{petigura-a, petigura}).

In this \paper, we develop a flexible framework for probabilistic inference of
exoplanet occurrence rates that can be applied to incomplete catalogs with
\emph{non-negligible observational uncertainties}.
Our method is a generalization of a previously studied method
(\citealt{hogge}) to account for detection efficiencies.
We run tests on simulated datasets---comparing results with the standard
techniques that neglect observational uncertainties---and apply our method to
a real catalog of small planets transiting Sun-like stars
(\citealt{petigura}).
For the purposes of this \paper, we make some strong assumptions---although we
argue that they are weaker than the implicit assumptions in previous
studies---but none of these are necessary for the applicability of our method.
Specifically, we make the following assumptions
\begin{itemize}

\item the candidates in the catalog are independent Poisson draws from the
censored occurrence rate,

\item every candidate is a real exoplanet (there are no false positives),

\item the observational uncertainties on the physical parameters are
non-negligible but known (the catalog provides probabilistic constraints on
the parameters),

\item the detection efficiency of the pipeline is known, and

\item the true occurrence rate function is \emph{smooth}\footnote{We give our
definition of ``smooth'' in more detail below but our model is very flexible
so this is not a strong restriction.}.

\end{itemize}
The first assumption---conditional independence of the candidates---is
reasonable since the dataset that we consider explicitly includes only single
transiting systems (\citealt{petigura}).
When considering other datasets it will be interesting and important to study
the distributions of taking multiple transiting systems into account
(\dfmtodo{CITE}) and this would be possible using a conditional distributions
(as developed by \citealt{tremaine}).
The second assumption---neglecting false positives---is also strong and not
really justified (\citealt{fressin-fp, morton}).
For this \paper, we will neglect this issue and only comment on the effects
but the prior distributions published by \citet{fressin-fp} would be directly
applicable in our method and this would be a very interesting generalization.

\section{A brief introduction to hierarchical inference}

In this \paper, we are asking the question: \emph{what constraints can we put
on the occurrence rate of exoplanets given all the light curves measured by
\kepler?}
Throughout this \paper, we use the notation $\rate_\ratepars(\entry)$ for the
occurrence rate \rate---parameterized by the parameters \ratepars---as a
function of the physical parameters \entry\ (orbital period, planetary radius,
\etc).
In this framework, the occurrence rate function can be ``parametric''---for
example, a power law---or a ``non-parametric'' function---such as a histogram
where the bin heights are the parameters \ratepars.
The light curve for a particular target $k$ is called $\data_k$ and the
\emph{true} set of physical parameters for that same object are given by
$\entry_k$.
A catalog provides a set of constraints on the parameters $\{\entry_k\}$
conditioned the set light curves $\{\data_k\}$.

Any inference about the rate function parameters can be formally addressed by
evaluating the \emph{marginalized likelihood function}
\begin{eqnarray}\eqlabel{crazylike}
p(\{\data_k\}\,|\,\ratepars) &=&
    \int p(\{\data_k\}\,|\,\{\entry_k\})
    \,p(\{\entry_k\}\,|\,\ratepars)
    \dd\{\entry_k\}
\end{eqnarray}
where, as we mentioned above, $\{\data_k\}$ is the set of all light curves,
one light curve $\data_k$ per target $k$, \ratepars\ is the vector of
parameters describing the occurrence rate function $\rate_\ratepars(\entry)$
and $\entry_k$ is the vector of physical parameters describing the planetary
system (orbital periods, radius ratios, stellar radius, \etc) around target
$k$.
In this equation, our only assumption is that the datasets depend on the
occurrence rate of exoplanets only through the catalog $\{\entry_k\}$.
In our case, this assumption qualitatively means that the signals found in the
light curves depend only on the actual planet properties and not on the
distributions from which they are drawn.
It is worth noting that---as we will discuss further below---the catalog only
provides \emph{probabilistic} constraints on $\{\entry_k\}$.

In other words, a catalog is a dimensionality reduction of the raw data with
all the relevant information retained.
In the context of \kepler, the catalog reduces the set of downloaded pixel
time series (approximately 70,000 data points for the typical \kepler\ target)
to probabilistic constraints on a handful of physical parameters---\entry\
from above---like the orbital period and planetary radius.
If we take this set of parameters $\{\entry_k\}$ as \emph{sufficient
statistics} of the data then we can, in theory, compute \eq{crazylike}---up to
an unimportant constant---without ever looking at the raw data again.
This is true because the catalog provides constraints on the posterior
probability of the parameters $\{\entry_k\}$ under some choice of ``interim
prior'' \interim
\begin{eqnarray}\eqlabel{crazypost}
p(\{\entry_k\}\,|\,\{\data_k\},\,\interim) &=&
\frac{p(\{\data_k\}\,|\,\{\entry_k\})\,p(\{\entry_k\}\,|\,\interim)}
     {p(\{\data_k\}\,|\,\interim)} \quad.
\end{eqnarray}
It turns out that we can use this posterior to simplify \eq{crazylike} to a
form that can, in many common cases, be evaluated efficiently.
To find this result, multiply the integrand in \eq{crazylike} by
\begin{eqnarray}
\frac{p(\{\entry_k\}\,|\,\{\data_k\},\,\interim)}
     {p(\{\entry_k\}\,|\,\{\data_k\},\,\interim)}
\end{eqnarray}
and use \eq{crazypost} to find
\begin{eqnarray}\eqlabel{simplemarglike}
\frac{p(\{\data_k\}\,|\,\ratepars)}{p(\{\data_k\}\,|\,\interim)} &=&
    \int
    \frac{p(\{\entry_k\}\,|\,\ratepars)}{p(\{\entry_k\}\,|\,\interim)}\,
    p(\{\entry_k\}\,|\,\{\data_k\},\,\interim)
    \dd\{\entry_k\} \quad.
\end{eqnarray}
The data only enter this equation through the posterior constraints provided
by the catalog $\{\entry_k\}$!
For our purposes, this is the \emph{definition} of hierarchical inference.

The constraints in \eq{crazypost} can always be---and often are---propagated
as a list of $N$ samples $\{\entry_k^{(n)}\}$ from the posterior
\begin{eqnarray}
\{\entry_k\}^{(n)} &\sim& p(\{\entry_k\}\,|\,\{\data_k\},\,\interim) \quad.
\end{eqnarray}
We can use these samples to approximately compute the integral in
\eq{simplemarglike} as
\begin{eqnarray}\eqlabel{importance}
\frac{p(\{\data_k\}\,|\,\ratepars)}{p(\{\data_k\}\,|\,\interim)} &\approx&
    \frac{1}{N} \sum_{n=1}^N
    \frac{p(\{\entry_k^{(n)}\}\,|\,\ratepars)}
         {p(\{\entry_k^{(n)}\}\,|\,\interim)} \quad.
\end{eqnarray}
This is very efficient to compute as long as an evaluation of
$p(\{\entry_k^{(n)}\}\,|\,\ratepars)$ is not very expensive.
That being said, this could be a high variance estimator depending on the
initial choice of $p(\{\entry_k^{(n)}\}\,|\,\interim)$ but in our case, we'll
show that not many samples are needed to get an accurate estimate.

A very simple example is the familiar procedure of making a histogram.
If you model the function $p(\{\entry_k\}\,|\,\ratepars)$ as a piecewise
constant rate function---where the bin heights are the parameters---and if the
uncertainties on the catalog are negligible compared to the bin widths then
the maximum marginalized likelihood solution for \ratepars\ is a histogram of
the catalog entries.
The case of non-negligible uncertainties is described by \citet{hogge} using a
method similar to the one discussed here.

\section{Model generalities}
\sectlabel{model}

In the inference procedure described above, we only discussed an abstract
occurrence rate function.
To make it more concrete, we'll model the catalog as a Poisson draw from the
\emph{observable} rate function $\obs{\rate}_\ratepars$.
This leads to the previously known result (see \citealt{tabachnik,youdin} for
some of the examples from the exoplanet literature)
\begin{eqnarray}\eqlabel{poisson-like}
p(\{\entry_k\}\,|\,\ratepars) &=&
    \exp\left(-\int \obs{\rate}_\ratepars (\entry) \dd\entry\right) \,
    \prod_{k=1}^K \obs{\rate}_\ratepars (\entry_k)\quad.
\end{eqnarray}
In this equation, the integral in the normalization term is the expected
number of observable exoplanets in the sample.
A further assumption that we have made in \eq{poisson-like} is that each
object is an independent sample from the rate function.
This assumption---though it is often made---cannot be correct when we consider
systems of multiple planets.
It is possible to relax this assumption (see \citealt{tremaine} for a related
technique) but this generalization is beyond the scope of the current \paper.

The main thing to note here is that $\obs{\rate}_\ratepars$ is the rate of
exoplanets that you would expect to observe taking into account the geometric
transit probability and any other detection efficiencies.
In practice, it probably makes sense to model the observable rate as
\begin{eqnarray}
\obs{\rate}_\ratepars(\entry) &=&
    \completeness(\entry)\,\rate_\ratepars(\entry)
\end{eqnarray}
where $\completeness(\entry)$ is the detection efficiency at \entry\ and
$\rate_\ratepars(\entry)$ is the object that we want to infer: the true
occurrence rate.
Up to this point, we haven't discussed any specific functional form for
$\rate_\ratepars(\entry)$ and all of this derivation is equally applicable
whether we model the occurrence rate as (for example) a broken power law or
a histogram.

For the results in this \paper, we will assume that the completeness function
$\completeness(\entry)$ is known empirically but that is not a requirement for
the validity of this method.
Instead, we could use a functional form for it and infer the parameters
(\dfmtodo{cite all the papers that do the ramp, step function, whatever}).

Now, we can substitute \eq{poisson-like} into \eq{simplemarglike} and apply
the importance sampling approximation to derive the following expression for
the marginalized likelihood
\begin{eqnarray}\eqlabel{money}
\frac{p(\{\data_k\}\,|\,\ratepars)}{p(\{\data_k\}\,|\,\interim)} &\approx&
    \exp\left(-\int \obs{\rate}_\ratepars (\entry) \dd\entry\right) \,
    \prod_{k=1}^K
    \frac{1}{N_k} \sum_{n=1}^{N_k}
    \frac{\obs{\rate}_\ratepars (\entry_k^{(n)})}
         {p(\entry_k^{(n)}\,|\,\interim)} \quad.
\end{eqnarray}
In this equation, we're making the further assumption that the catalog treated
the objects independently.
In other words, we have per-object posterior samples
\begin{eqnarray}
\entry_k^{(n)} &\sim& p(\entry_k\,|\,\data_k,\,\interim) \quad.
\end{eqnarray}
\Eq{money} is the \emph{money equation} for our method.
It lets us efficiently compute the \emph{marginalized likelihood of the entire
set of light curves for a particular occurrence rate function}.

\paragraph{Inverse-detection-efficiency}
It's now interesting to take a brief aside and discuss the connection between
our model and the commonly used inverse-detection-efficiency procedure
(\citealt{howard,dressing,petigura}).
This procedure involves making a weighted histogram of the catalog entries
where the weight for object $\entry_k$ is $1/\completeness(\entry_k)$.
This makes intuitive sense but (to our knowledge) does not have a clear
probabilistic justification or interpretation.

If we model the occurrence rate as a histogram with $J$ fixed bin volumes
$\binarea_j$
\begin{eqnarray}
\rate_\ratepars (\entry) &=& \left\{\begin{array}{ll}
\ratepar_1 & \entry \in \binarea_1,\\
\ratepar_2 & \entry \in \binarea_2,\\
\cdots \\
\ratepar_J & \entry \in \binarea_J,\\
0 & \mathrm{otherwise}
\end{array}\right.
\end{eqnarray}
then \eq{poisson-like} becomes
\begin{eqnarray}
\ln p(\{\entry_k\}\,|\,\ratepars) &=&
    \sum_{k=1}^K \sum_{j=1}^J \mathbf{1}[\entry_k \in
        \binarea_j]\,\ln\completeness(\entry_k)\,\ratepar_j
    -\sum_{j=1}^J\ratepar_j\,\int_{\binarea_j} \completeness(\entry)\dd\entry
\end{eqnarray}
where the indicator function $\mathbf{1}[\cdot]$ is one if $\cdot$ is true and
zero otherwise.
If we take the gradient of this function with respect to \ratepars\ and set it
equal to zero, we find the maximum likelihood result
\begin{eqnarray}
\ratepar_j &=& \frac{N_j}{\int_{\binarea_j} \completeness(\entry)\dd\entry}
\end{eqnarray}
where $N_j$ is the number of objects that fall within the bin $j$.
Since it is the maximum likelihood solution, this result is the minimum
variance estimator for the occurrence rate in the limit of negligible catalog
uncertainties.
This is \emph{almost} the same as the inverse-detection-efficiency result and
in various limiting cases, they would provide the same result.
\dfmtodo{Finish explaining why this result is better under the specified
assumptions.}

\paragraph{occurrence rate parameterization \& priors}
For the remainder of this \paper, we also model the occurrence rate as a
two-dimensional histogram with fixed logarithmic bins in period and radius.
When we include observational uncertainties---using \eq{money}---the maximum
likelihood result is no longer analytic.
Therefore, if we want to compute the ``best-fit'' occurrence rate, we can use a
standard non-linear optimization algorithm.
In the regions of parameter space that we tend to care about, the completeness
is low and there are only a few observations with large uncertainties.
In this case, we're especially interested in probabilistic constraints on the
occurrence rate; not just the best-fit model.
To do this, we apply a prior\footnote{This is a slight misnomer because
we also we also marginalize over the parameters of this model.} $p(\ratepars)$
on the occurrence rate parameters and generate samples from the posterior
probability
\begin{eqnarray}\eqlabel{posterior}
p(\ratepars\,|\,\{\data_k\}) &\propto&
    p(\ratepars)\,p(\{\data_k\}\,|\,\ratepars)
\end{eqnarray}
using Markov chain Monte Carlo (MCMC).

Given our model of the rate function as a collection of bins, we choose to
model the prior as a Gaussian process (\citealt{gp,gibson-gp}) on the
logarithmic bin heights.
This model encodes our prior belief that, on the grid scale that we consider,
the occurrence rate should be smooth but it is otherwise very flexible about
the form of the function.
Mathematically, the prior is written as
\begin{eqnarray}
p(\ratepars) &=& p(\ratepars\,|\,\mean,\,\smooth) \\
&=& \mathcal{N} \left[\ln\ratepars;\,\mean\,\bvec{I},\,
\cov(\{\binarea_j\},\,\smoothpars)\right]
\end{eqnarray}
where $\mathcal{N}(\cdot;\,\mean\,\bvec{I},\,\cov)$ is a $J$-dimensional
Gaussian with a constant mean \mean\ and covariance matrix \cov\ that depends
on the bin locations $\{\binarea_j\}$ and a set of hyperparameters
$\smoothpars = (\smooth_0,\,\smooth_\period,\,\smooth_\radius)$.
The covariance function that we use is an anisotropic, axis-aligned
exponential-squared kernel so elements of the matrix are
\begin{eqnarray}
\cov_{ij} &=& \smooth_0\,\exp\left(-\frac{1}{2}\,
    [\binarea_i-\binarea_j]^\mathrm{T}\,\Sigma^{-1}\,[\binarea_i-\binarea_j]
\right)
\end{eqnarray}
where $\Sigma^{-1}$ is the diagonal matrix
\begin{eqnarray}
\Sigma^{-1} &=& \left(\begin{array}{cc}
1/\smooth_\period & 0 \\
0 & 1/\smooth_\radius
\end{array}\right) \quad.
\end{eqnarray}

As well as being a very flexible prior, this Gaussian process model is also
appealing because it is well studied in the machine learning literature.
In particular, there is a very efficient algorithm---elliptical slice sampling
(ESS; \citealt{ess})---for generating samples from the posterior probability
function in \eq{posterior}.
As mentioned above, we actually go one step further and assume uniform
(hyper-)priors on $\ln\mean$ and $\ln\smoothpars$ in order to sample from the
joint posterior
\begin{eqnarray}
p(\ratepars,\,\mean,\,\smoothpars\,|\,\{\data_k\})
\end{eqnarray}
using Algorithm 1 from \citet{ess-hyper}.

For all the results below, we run a Markov chain with $10^6$ steps for the bin
heights and update the hyperparameters every 10 steps.
We discard the first \dfmtodo{XXXX} samples as burn-in and then use the
remaining samples as an approximation to the marginalized probability
distribution for \ratepars.
We tune the Metropolis--Hastings proposal used to update the hyperparameters
by hand to get an acceptance fraction of $\sim0.2-0.4$ for those parameters.

\section{Data and completeness function}
\sectlabel{data}

Using an independent exoplanet search and characterization pipeline,
\citet{petigura} published a catalog of 603 planet candidates orbiting stars
in their ``Sun-like'' sample of \kepler\ targets.
For each candidate, \citet{petigura} used Markov chain Monte Carlo to sample
the posterior probability density---presumably assuming uniform priors---for
the radius ratio, transit duration, and impact parameter.
They then incorporated the uncertainties in the stellar radius and published
constraints on the physical radii of their candidates.
Given this data reduction and since we don't have access to the individual
posterior constraints on radius ratio and stellar radius, we can't directly
compute the importance weights $p(\{\entry_k^\}\,|\,\interim)$ needed for
\eq{importance}.
For the rest of this \paper, we'll make the simplifying assumption that these
weights are constant in log-period and log-radius but the results don't seem
to be sensitive to this specific choice.

A huge benefit of this dataset is that Erik Petigura and collaborators
published a rigorous analysis the empirical end-to-end completeness of their
transit search pipeline.
Instead of choosing a functional form for the detection efficiency of the
pipeline as a function of the parameters of interest, \citet{petigura}
injected synthetic signals of known period and radius into the raw aperture
photometry and determining the empirical recovery rate after the full analysis.

We use all the injected samples from \citet{petigura} to compute the mean
(marginalized) detection efficiency in bins of $\ln\period$ and $\ln\radius$.
In each bin, this efficiency is simply the fraction of recovered injections.
For the purposes of this \paper, we neglect the counting uncertainties
introduced by the finite number of samples.

The largest injected signal had a radius of $16\,R_\oplus$ but, because of the
measurement uncertainties on the radii, we need to model the distribution at
larger radii.
To do this, we approximate the survey completeness for $\radius>16\,R_\oplus$
as 1.

Given our domain knowledge of how detection efficiency depends on the physical
parameters, it might seem silly to measure the survey completeness in period
and radius instead of radius ratio or signal-to-noise.
It is also likely that a change of coordinates would yield a higher precision
result.
That being said, it is still correct to measure the completeness in period and
radius, and there are a few practical reasons for our choice.
The main argument is that since the radius uncertainties are dominated by
uncertainties in the stellar parameters, it is not possible to use the
published catalog (\citealt{petigura}) to compute constraints on radius
ratios.
In the future, this problem would be solved by publishing a representation of
\emph{the full posterior PDF for each object in the catalog}.
In this case, the most useful data product would be \emph{posterior samples
for each target's radius ratio and stellar radius}.

The detection efficiency also depends on the geometric transit probability
$R_\star/a$.
Since we're modeling the distribution in the period--radius plane, we need to
compute the transit probability marginalized over stellar radius and mass.
This marginalized distribution scales only with the period of the orbit as
$\propto \period^{-2/3}$.
In theory, this marginalization should be over the true distribution of these
parameters in the selected stellar catalog but we'll approximate it by
the empirical distribution; a reasonable approximation given the size of the
dataset.
At a period of 10 days, the median transit probability in the selected sample
of stars is $5\%$ so we model the transit probability\footnote{We're using the
letter $Q$ to indicate probabilities since we're already using $P$ to mean
period.} as a function of period as
\begin{eqnarray}
\transitprob (\period) &=&
    0.05\,\left[\frac{\period}{10\,\mathrm{days}}\right]^{-2/3} \quad.
\end{eqnarray}
This expression is clearly only valid for $\period \gtrsim 1.4\,\mathrm{days}$
but the dataset that we're using (\citealt{petigura}) explicitly only includes
periods longer than five days so this isn't a problem.
It's also worth noting that we're using the \emph{median} transit probability
(instead of the mean) because it is a more robust estimator in the presence of
outliers.
In our experiments, the results do not seem to be very sensitive to this
choice.
\dfmtodo{Is there any rough scaling that is obvious?}

\section{Validation using synthetic catalogs}

In order to get a feeling for the constraints provided by our method and to
explore any biases introduced by ignoring the observational uncertainties, we
start by ``observing'' a few synthetic catalogs from qualitatively different
known occurrence rate functions.
For each of these simulations, we'll take the completeness function computed
by \citet{petigura} as given.
In general, \eq{poisson-like} can be sampled using a procedure called thinning
(\citealt{poisson}) but for our purposes, we'll simply consider a piecewise
constant rate function evaluated on a fine grid in log-period and log-radius.
For this discrete function, the generative procedure is simple;
\begin{enumerate}
{\item loop over each grid cell $i$,}
{\item draw Poisson random integer $K_i\sim\mathrm{Poissson}(\obs{\rate}_i)$
with the observable rate evaluated in the cell, and}
{\item distribute $K_i$ catalog entries in the cell randomly.}
\end{enumerate}
We then choose fractional observational uncertainties on the radii from the
\citet{petigura} catalog and apply them to the true catalog as Gaussian noise.

We generate synthetic catalogs from two qualitatively different rate
functions.
Both distributions are generated by a separable model
\begin{eqnarray}
\rate_\ratepars (\ln\period,\,\ln\radius) &=&
    \rate_\ratepars^{(\period)}(\ln\period)\,
    \rate_\ratepars^{(\radius)}(\ln\radius)
\end{eqnarray}
but fit using the full general model.
The first catalog---\modela--- is generated assuming a smooth occurrence
surface where both distributions are broken power laws.
The second---\modelb---is designed to be exactly the distribution inferred by
\citet{petigura} in the range that they considered and then smoothly
extrapolated outside that range.
The catalogs generated from these two models are shown in \fig{smooth-results}
and \fig{simulation-results}, respectively and the data are available online
at \dfmtodo{ADD SOME URL}.

For each catalog, we directly apply the inverse-detection-efficiency procedure
as implemented by \citealt{petigura}\footnote{Our implementation reproduces
their results when applied to the published catalog.} and our probabilistic
method, marginalizing over the hyperparameters of the Gaussian process
regularization.
\Fig{smooth-results} and \fig{simulation-results} show the results of this
analysis in both cases.
In particular, the side panels compare the marginalized occurrence rate
functions in period and radius to the true functions that were used to
generate the catalogs.
\Fig{smooth-results} shows that even if the true occurrence rate is a smooth
function, the rate inferred by the inverse-detection-efficiency method can
appear to have sharp features.
In this first example---where the true distribution is well described by our
Gaussian process model---the probabilistic inference of the occurrence rate is
both more precise and accurate.

In the second example, the true rate function includes a sharp feature chosen
to reproduce the result published by \citet{petigura}.
In this case, \fig{simulation-results} shows that the probabilistic
constraints on the occurrence rate are less precise \emph{but more accurate}
than results using the inverse-detection-efficiency method.
At this point, the distributions inferred by these two different techniques
don't seem drastically inconsistent but the difference are most obvious as the
survey completeness goes to zero: exactly the region of interest.

\begin{figure}[p]
\begin{center}
\includegraphics[width=\textwidth]{figures/smooth/results.pdf}
\end{center}
\caption{%
{\bf Simulated data}.
Inferences about the occurrence rate function based on the simulated catalog
\modela.
\emph{Center:} the points with error bars show the exoplanet candidates in the
simulated incomplete catalog, the contours show the survey completeness
function (\citealt{petigura}), and the grayscale shows the median posterior
occurrence surface.
\emph{Top and left:} the red dashed line shows the \emph{true} distribution
that was used to generate the catalog, the points with error bars show the
results of the inverse-detection-efficiency procedure, and the histograms are
posterior samples from the marginalized occurrence rate as inferred by our
method.
\figlabel{smooth-results}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=\textwidth]{figures/simulation/results.pdf}
\end{center}
\caption{%
{\bf Simulated data}.
The same as \fig{smooth-results} for \modelb.
\figlabel{simulation-results}}
\end{figure}

\section{Extrapolation to Earth}
\sectlabel{extrap}

As well as inferring the ocurrence distribution of exoplanets, this dataset
can also be used to constrain the rate of Earth analogs.
Explicitly, we constrain the rate of exoplanets \emph{evaluated at the
location of Earth}
\begin{eqnarray}
\gammaearth &=& \rate (\ln\period_\oplus,\,\ln\radius_\oplus) \\
&=&
\left.\frac{\dd N}{\dd\ln\period\,\dd\ln\radius}\right|
_{\radius=\radius_\oplus,\,\period=\period_\oplus}\quad.
\end{eqnarray}
This is also the quantity that \citet{petigura} estimated but in different
units.
That is, \gammaearth\ is the average number of exoplanets around a
``Sun-like'' star---per natural logarithmic bins in period and
radius---evaluated at the period and radius of Earth.

Since no Earth analogs have been found any constraints on this rate must be
extrapolated from the existing observations.
This extrapolation is generally done by assuming a functional form for the
occurrence rate function, constraining it using the observed candidates and
extrapolating.
All published extrapolations are based on rigid models like a power law
(\citealt{catanzarite, traub}) or linear in the cumulative period distribution
(\citealt{petigura}).
All of these results are sensitive to this choice of function and to the other
implicit decisions (for example the size of radius bin that is considered
Earth-like by \citealt{petigura}).
We weaken the assumptions necessary for extrapolation by only assuming that
the distribution is smooth using the Gaussian process prior described in
\sect{model}.
This model will, of course, break down if there is a sharp feature in the
occurrence rate function right at the location of Earth but otherwise this is
the most conservative extrapolation method published up to this point.

The results of our extrapolation method, when applied to the synthetic
catalogs described in the previous section, are shown in \fig{smooth-rate} and
\fig{simulation-rate}.
We implemented the extrapolation method from \citet{petigura} that assumes a
linear relation in the cumulative period distribution for small planets and
compared these results---converting to the correct units---to our method and
the truth.
In both cases, our method returns a less precise but more accurate result for
the occurrence rate and the error bars given by the functional extrapolation
are overly optimistic.

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/smooth/rate.pdf}
\end{center}
\caption{%
{\bf Simulated data}.
The extrapolated rate of Earth analogs \gammaearth\ as inferred by the
different techniques applied to the \modela\ simulation.
Applying the method used by \citet{petigura} gives a constraint indicated by
the vertical black line with error bars shown as dashed lines.
The histogram is the MCMC estimate of our posterior constraint on this rate
and the true value is indicated as the thick gray vertical line.
\figlabel{smooth-rate}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/simulation/rate.pdf}
\end{center}
\caption{%
{\bf Simulated data}.
The same as \fig{smooth-rate} for \modelb.
\figlabel{simulation-rate}}
\end{figure}

\section{Results from real data}

Having developed this probabilistic framework for occurrence rate inferences
and demonstrating that it produces reasonable results when applied to
simulated datasets, we now turn to real data.
As described in \sect{data}, we will use the catalog of small exoplanet
candidates orbiting Sun-like stars published by \citet{petigura}.
This is a great test case because those authors empirically measured the
detection efficiency of their pipeline as a function of the parameters of
interest.

\Fig{real-results} shows posterior samples from the inferred occurrence rate
distribution as a function of period and radius conditioned on the catalog.
The marginalized distributions are qualitatively consistent with the
occurrence rate functions measured by \citet{petigura}.
The MCMC chain used to generate this figure is available online at
\dfmtodo{ADD SOME URL}.

Our constraint on the rate of Earth analogs (as defined in \sect{extrap}) is
in tension---even though our result has large fractional uncertainty---with
the result from \citet{petigura}.
This is shown in \fig{real-rate} where we compare the marginalized posterior
PDF for \gammaearth\ to the published value and uncertainty.
Quantitatively, we find that the rate of Earth analogs---the average number of
Earth-sized planets orbiting a Sun-like on a year-long period---is
\begin{eqnarray}
\gammaearth &=& 0.017^{+0.018}_{-0.009} \quad.
\end{eqnarray}
Converted to these units, \citet{petigura} measured $0.119_{-0.046}^{+0.035}$
for the same quantity (indicated as the vertical lines in \fig{real-rate}).

\begin{figure}[p]
\begin{center}
\includegraphics[width=\textwidth]{figures/results/results.pdf}
\end{center}
\caption{%
The same as \fig{smooth-results} when applied to the observed data from
\citet{petigura}.
\figlabel{real-results}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/results/rate.pdf}
\end{center}
\caption{%
The same as \fig{smooth-rate} but applied to the catalog from \citet{petigura}.
\figlabel{real-rate}}
\end{figure}

\section{Discussion}

In our analysis we make a few simplifying assumptions.
Each of these could be relaxed and \dfmtodo{BLAH BLAH BLAH}.
\begin{itemize}

\item {\bf Conditional independence}\quad
We assumed that every object in the catalog was a conditionally independent
Poisson draw from the observable occurrence rate function.
This is an interesting assumption to consider when applying this method to a
different catalog where multiple transiting systems are included.
In practice, the best first step towards relaxing this assumption is probably
to follow \citet{tremaine} and assume that the mutual inclination distribution
is the only source of conditional dependence between planets.
For this \paper, the assumption of conditional independence is justified
because the dataset explicitly includes only systems with a single transiting
exoplanet.

\item {\bf False positives}\quad

\item {\bf Known observational uncertainties}\quad

\item {\bf Given empirical detection efficiency}\quad

\item {\bf Smooth rate function}\quad

\end{itemize}

% We have made the first measurement of the Earth-like exoplanet population that
% is based on forward modeling, takes proper account of the observational
% uncertainties on planet radii, and makes use of extremely flexible
% non-separable distributions for period and radius.
% We find \hoggtodo{BLAH BLAH BLAH.}

% Although our method makes weaker assumptions than any previous work in this
% area, of course it \emph{does} make strong assumptions.
% It assumes that the data set (from \citealt{petigura}) contains planets and
% only planets (no false positives).
% It assumes that the individual-planet radius uncertainties are also correctly
% determined.
% It assumes that the completeness calculation (again from \citealt{petigura})
% is accurate and smoothly varying with period and radius; it makes no attempt
% to adjust the completeness star-by-star for brightness or variability.
% It assumes that the exoplanets are all independently drawn from the same
% distribution function, with no regard to age, metallicity, or multiplicity.

% Although we have taken account of the radius uncertainties in a principled
% way, we did make one important approximation: We ignored planets with central
% values far outside the period and radius box in which we built the model.
% That is, we assumed that there are no very large outliers in radius
% determination.
% This assumption is a complex one; it involves the effective, end-to-end error
% distribution in the data set, but also smoothness of the distribution function
% at or near the edges of the analysis box.

% If our inferences are right about true Earth analogs, then we expect that
% there are about XXX transiting Earth analogs hidden in the \kepler\ data set.
% We would be able to find some of these Earth analogs if the effective noise
% level in the data could be reduced by a factor of about XXX.
% We have identified a number of directions along which this noise reduction
% might happen, including but not limited to retrospective data-driven or
% physics-driven recalibrations of the \kepler\ focal plane
% (\citealt{hoggwhitepaper}), new methods for performing aperture photometry,
% and better models for stochastic instrument variability or stochastic
% intrinsic stellar variability that take account of frequency structure
% (\citealt{brewer, carter, roberts}).
% If any (or all) of these deliver substantial improvements for \kepler\
% archival data reanalyses, it's Nobel Prize time!  (Or at least \textsl{The
% Colbert Report}?)

All of the code used in this project is available from
\url{http://github.com/dfm/exopop} under the MIT open-source software license.
This code (plus some dependencies) can be run to re-generate all of the
figures and results in this \paper; this version of the paper was generated
with git commit \texttt{\githash} (\gitdate).

\acknowledgments
We would like to thank Erik Petigura (Berkeley) for freely sharing his data
and code.
It is a pleasure to thank
\ldots
for helpful contributions to the ideas and code presented here.
This project was partially supported by the NSF (grant AST-0908357), and NASA
(grant NNX08AJ48G).
This research builds on collaborations between astronomers and statisticians
forged during a three week workshop on ``Modern Statistical and Computational
Methods for Analysis of Kepler Data'' at SAMSI in June 2013.
This research made use of the NASA \project{Astrophysics Data System}.

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\begin{thebibliography}{}\raggedright

\bibitem[Adams, Murray \& MacKay(2009)]{poiss-gp}
Adams, R.~P., Murray, I., \& MacKay, D.~J.~C.\ 2009, ICML, 2009, 9
(\href{http://homepages.inf.ed.ac.uk/imurray2/pub/09poisson/}{online})

\bibitem[Brewer \& Stello(2009)]{brewer}
Brewer, B.~J., \& Stello, D.\ 2009, \mnras, 395, 2226 (\arxiv{0902.3907})

\bibitem[Carter \& Winn(2009)]{carter}
Carter, J.~A., \& Winn, J.~N.\ 2009, \apj, 704, 51 (\arxiv{0909.0747})

\bibitem[Catanzarite \& Shao(2011)]{catanzarite}
Catanzarite, J., \& Shao, M.\ 2011, \apj, 738, 151 (\arxiv{1103.1443})

\bibitem[Dong \& Zhu(2013)]{dong}
Dong, S., \& Zhu, Z.\ 2013, \apj, 778, 53 (\arxiv{1212.4853})

\bibitem[Dressing  \& Charbonneau(2013)]{dressing}
Dressing, C.~D., \& Charbonneau, D.\ 2013, \apj, 767, 95 (\arxiv{1302.1647})

\bibitem[Fressin \etal(2013)]{fressin-fp}
Fressin, F., Torres, G., Charbonneau, D., \etal\ 2013, \apj, 766, 81
(\arxiv{1301.0842})

\bibitem[Gibson \etal(2012)]{gibson-gp}
Gibson, N.~P., Aigrain, S., Roberts, S., \etal\ 2012, \mnras, 419, 2683

\bibitem[Hogg \etal(2013)]{hoggwhitepaper}
Hogg, D.~W., Angus, R., Barclay, T., et al.\ 2013, \arxiv{1309.0653}

\bibitem[Hogg \etal(2010)]{hogge}
Hogg, D.~W., Myers, A.~D., \& Bovy, J.\ 2010, \apj, 725, 2166
(\arxiv{1008.4146})

\bibitem[Howard \etal(2012)]{howard}
Howard, A.~W., Marcy, G.~W., Bryson, S.~T., et al.\ 2012, \apjs, 201, 15
(\arxiv{1103.2541})

\bibitem[Lewis \& Shedler(1979)]{poisson}
Lewis, P.~A.~W., \& Shedler, G.~S.\ 1979, Naval Research Logistics Quarterly,
26, 403

\bibitem[Morton \& Johnson(2011)]{morton}
Morton, T.~D., \& Johnson, J.~A.\ 2011, \apj, 738, 170 (\arxiv{1101.5630})

\bibitem[Morton \& Swift(2013)]{morton-swift}
Morton, T.~D., \& Swift, J.~J.\ 2013, \arxiv{1303.3013}

\bibitem[Murray \etal(2010)]{ess}
Murray, I., Prescott Adams, R., \& MacKay, D.~J.~C.\ 2010, JMLR: W\&CP, 9, 541
(\arxiv{1001.0175})

\bibitem[Murray \& Prescott Adams(2010)]{ess-hyper}
Murray, I., \& Prescott Adams, R.\ 2010, Advances in Neural Information
Processing Systems, 23, 1723
(\arxiv{1006.0868})

\bibitem[Petigura \etal(2013a)]{petigura-a}
Petigura, E.~A., Marcy, G.~W., \& Howard, A.~W.\ 2013a, \apj, 770, 69
(\arxiv{1304.0460})

\bibitem[Petigura \etal(2013b)]{petigura}
Petigura, E.~A., Howard, A.~W., \& Marcy, G.~W.\ 2013b,
Proceedings of the National Academy of Science, 110, 19273 (\arxiv{1311.6806})

\bibitem[Rasmussen \& Williams(2006)]{gp}
Rasmussen, C.~E. \& Williams, C.~K.~I.\ 2006
Gaussian Processes for Machine Learning, MIT Press
(\href{http://www.gaussianprocess.org/gpml/}{online})

\bibitem[Roberts \etal(2013)]{roberts}
Roberts, S., McQuillan, A., Reece, S., \& Aigrain, S.\ 2013, \mnras, 435, 3639
(\arxiv{1308.3644})

\bibitem[Tabachnik \& Tremaine(2002)]{tabachnik}
Tabachnik, S., \& Tremaine, S.\ 2002, \mnras, 335, 151
(\arxiv{astro-ph/0107482})

\bibitem[Traub(2012)]{traub}
Traub, W.~A.\ 2012, \apj, 745, 20 (\arxiv{1109.4682})

\bibitem[Tremaine \& Dong(2012)]{tremaine}
Tremaine, S., \& Dong, S.\ 2012, \aj, 143, 94 (\arxiv{1106.5403})

\bibitem[Youdin(2011)]{youdin}
Youdin, A.~N.\ 2011, \apj, 742, 38 (\arxiv{1105.1782})

\end{thebibliography}

\end{document}
