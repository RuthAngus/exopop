%
%  RULES OF THE GAME
%
%  * 80 characters
%  * line breaks at the ends of sentences
%  * eqnarrys ONLY
%  * ALWAYS cite inside parens () and use \citealt{}; no in-line cites
%  * that is all.
%

\documentclass[12pt,preprint]{aastex}

\include{vc}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{subfigure}

\newcommand{\project}[1]{{\sffamily #1}}
\newcommand{\emcee}{\project{emcee}}
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\license}{MIT License}

\newcommand{\paper}{\emph{Article}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\bvec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2} \emph{#1} TODO: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\hoggtodo}[1]{\todo{HOGG}{blue}{#1}}
\newcommand{\mortontodo}[1]{\todo{MORTON}{green}{#1}}

% Document specific variables.
\newcommand{\rate}{\ensuremath{\Gamma}}
\newcommand{\ratepar}{{\ensuremath{\theta}}}
\newcommand{\ratepars}{{\ensuremath{\bvec{\ratepar}}}}
\newcommand{\obs}[1]{\ensuremath{\hat{#1}}}
\newcommand{\radius}{\ensuremath{R}}
\newcommand{\period}{\ensuremath{T}}
\newcommand{\completeness}{{\ensuremath{P_\mathrm{c}}}}
\newcommand{\transitprob}{{\ensuremath{P_\mathrm{t}}}}
\newcommand{\data}{{\ensuremath{\bvec{x}}}}
\newcommand{\entry}{{\ensuremath{\bvec{w}}}}
\newcommand{\catalog}{{\ensuremath{\bvec{\entry}}}}

\newcommand{\interim}{{\ensuremath{\bvec{\alpha}}}}

\newcommand{\binarea}{{\ensuremath{\Delta}}}
\newcommand{\bincenter}{{\ensuremath{\bvec{x}}}}
\newcommand{\binheight}{{\ensuremath{w}}}
\newcommand{\binheights}{{\ensuremath{\bvec{\binheight}}}}

\newcommand{\smooth}{{\ensuremath{\lambda}}}

\begin{document}

\title{%
  Inferring the distribution and rate of Earth-like exoplanets \\
  from noisy individual-planet inferences
}

\newcommand{\nyu}{2}
\newcommand{\mpia}{3}
\newcommand{\princeton}{4}
\newcommand{\berkeley}{5}
\author{%
    Daniel~Foreman-Mackey\altaffilmark{1,\nyu},
    David~W.~Hogg\altaffilmark{\nyu,\mpia},
    Timothy~D.~Morton\altaffilmark{\princeton},
    \etal
}
\altaffiltext{1}         {To whom correspondence should be addressed:
                          \url{danfm@nyu.edu}}
\altaffiltext{\nyu}      {Center for Cosmology and Particle Physics,
                          Department of Physics, New York University,
                          4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{\mpia}     {Max-Planck-Institut f\"ur Astronomie,
                          K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{\princeton}{Department of Astrophysics, Princeton University,
                          Princeton, NJ, 08540, USA} % CHECK THIS ZIP CODE

\begin{abstract}
Although no true extra-solar Earth analog is known, hundreds of planets have
been found around Sun-like stars that are either Earth-sized but on shorter
periods, or else on year-long orbits but somewhat bigger than Earth.
These populations ought to permit a (possibly highly uncertain) extrapolated
measurement of the probability or rate at which Sun-like stars host true Earth
analogs.
Here we present a general probabilistic framework for making justified
inferences about the occurrence rate of exoplanets taking into account survey
completeness \emph{and observational uncertainties}.
We demonstrate that our method produces lower variance estimates than the
standard inverse-detection-efficiency weighting procedure.
When applied to an existing catalog of small planet candidates around G dwarfs
(Petigura \etal\ 2013), our method confirms previous measurements of a
flattening radius distribution near Earth's radius.
One output of our models is fully marginalized estimates---marginalizing out
observational uncertainties and all distribution parameters---of the rate
density of true Earth analogs, expressed as a number of planets per star per
natural logarithmic interval of period and radius, evaluated at the properties
of Earth.
The rate density we infer is an extrapolation; it depends strongly on the
permitted flexibility of the distribution model, but it tends to come out
at around ten percent, with large uncertainty.
\end{abstract}

\keywords{%
exoplanets: sickness
---
exoplanets: eta-Earth
---
code: open-source
---
keywords: made-up-by-Hogg
}

\section{Introduction}

\hoggtodo{%
\kepler\ has been busting out exoplanets (CITE).
Many of them even appear possibly rocky and habitable (CITE).
Nonetheless, none yet known are true Earth analogs.
}

\hoggtodo{%
This hasn't stopped anyone from estimating the rate at which Sun-like stars
host Earth-like planets on year-ish period orbits (CITE).
All of these rate estimates are extrapolations of one kind or another.
Here we perform an extremely conservative extrapolation of this kind.
}

\hoggtodo{%
What would make an extrapolation like this conservative?
Using \emph{huge bins} does not a conservative estimate make.
You don't want to assume separabilities you can't justify.
You want to explore an enormous function space for the exoplanet rates or pdf.
We are going to do be conservative in these senses here, far more conservative
than anyone who has come before.
}

\dfmtodo{%
Cite \citet{tremaine} for likelihood function including multiplicity.
}

This study is novel in a number of ways.
We create probabilistic information about the full exoplanet population
\emph{not} by weighting the detected objects by the inverse detection
efficiency (\citealt{petigura}), but rather by forward modeling the
observed distribution subject to the detection efficiency
(\dfmtodo{CITE TREMAINE?} \citealt{dong}).
Re-weighting the data by inverse selection probability---a method called
V-max in the quasar and galaxy luminosity function contexts---is not wrong,
but it produces a higher-variance estimate of the population than a justified
likelihood approach; it is higher variance because
the effective number of samples, after
weighting, can be much smaller than the actual number of samples.
In this work, we build a justified
likelihood of the observed data, taking into account the completeness function
or detection efficiency.
Our likelihood function is a variable-rate Poisson likelihood; it makes the
fundamental assumption that the data points are independently drawn from the
model, but is agnostic about all other properties of the data or distribution
functions.
Indeed, the assumptions behind this study here are weaker than those of any
previous study of the period and radius distribution of the Earth-sized
exoplanet population (\dfmtodo{CITE TREMAINE?} \citealt{dong, petigura}).

Another new aspect of this study is that it makes use of very flexible
distribution function models.
Indeed most of the models we consider are ``non-parametric'' in the weak sense
that they have enormous numbers of free parameters (not the strong sense of
having an \emph{infinite} number of free parameters).
The models are protected from degeneracies and over-fitting by priors.
In some models we assume that the two-dimensional distribution of planets in
period and radius is separable; in others we don't.
We treat all of these parameters as ``nuisance parameters'' when we ask
questions about the occurrence of Earth analogs.
That is, our rate conclusions are fully marginalized, and those
marginalizations are over non-trivial numbers of parameters.

When, in what follows, we ask questions about the radius disribution of
exoplanets, the distribution parameters are no longer nuisances The
radius-distribution parameters are what we are trying to measure.
Even in this case, however, our results will have the period-distribution
parameters marginalized out.

Perhaps the most important novelty of this study is in its treatment of
observational uncertainties.
Although planet periods in our data set are very precisely measured, the
planet radius measurements have large uncertainties.
The justified probabilistic approach to including these uncertainties in a
distribution analysis is to introduce the ``true'' radii as latent variables
and infer and marginalize them out along with all other nuisance parameters.
Although this sounds expensive---and it can be---there is a simple approach
we have been advocating (\cite{hogge}) that makes use of importance sampling.
We use this approach here; our results are fully marginalized over posterior
uncertainties in all of the planet radii.
More will be said about this below; the importance sampling looks like a
product over data points of sums (averages) over individual-datum radius
samplings.

\section{A brief introduction to hierarchical inference}

In this \paper, we are asking the question: \emph{what constraints can we put
on the occurence rate of exoplanets given all the light curves measured by
\kepler?}
Mathematically, this involves evaluating the so-called \emph{marginalized
likelihood function}
\begin{eqnarray}\eqlabel{crazylike}
p(\{\data_k\}\,|\,\ratepars) &=&
    \int\left[\prod_{k=1}^K p(\data_k\,|\,\entry_k)\right ]
    \,p(\{\entry_k\}\,|\,\ratepars)
    \dd\{\entry_k\}
\end{eqnarray}
where $\{\data_k\}$ is the set of all light curves, \ratepars\ is a vector of
parameters describing the occurence rate function $\rate_\ratepars(\entry)$
and \entry\ is a vector of physical parameters describing a planetary system
(orbital periods, radius ratios, stellar radius, \etc).
In this equation, we have only made one assumption ...

It is generally computationally intractable to compute the likelihood in
\eq{crazylike} because it involves a high dimensional numerical integration of
a function where each evaluation of the integrand touches \emph{every data
point}.
Instead, we can use existing catalogs of planet candidates to approximate the
marginalization integral.
For our purposes, this is the definition of \emph{hierarchical inference}.

A catalog is a dimensionality reduction of the raw data that with all the
relevant information retained (we hope!).
In the context of \kepler, the catalog reduces the set of downloaded pixel
time series (approximately XXXX data points for the average \kepler\ target)
to probabilistic constraints on a handful of physical parameters---\entry\
from above---like the orbital period and radius.
If we take this set of parameters as \emph{sufficient statistics} of the data
then we can, in theory, compute \eq{crazylike}---up to an unimportant
constant---without ever looking at the raw data again.
This is true because the catalog provides constraints on the posterior
probability of the parameters $\{\entry_k\}$ under some choice of ``interim
prior'' \interim
\begin{eqnarray}\eqlabel{crazypost}
p(\{\entry_k\}\,|\,\{\data_k\},\,\interim) &=&
\frac{p(\{\entry_k\}\,|\,\interim)}{p(\{\data_k\}\,|\,\interim)}\,
\prod_{k=1}^K p(\data_k\,|\,\entry_k) \quad.
\end{eqnarray}
It turns out that we can use this posterior to simplify \eq{crazylike} to a
form that can, in many common cases, be evaluated efficiently.
To find this result, multiply the integrand in \eq{crazylike} by
\begin{eqnarray}
\frac{p(\{\entry_k\}\,|\,\{\data_k\},\,\interim)}
     {p(\{\entry_k\}\,|\,\{\data_k\},\,\interim)}
\end{eqnarray}
and use \eq{crazypost} to find
\begin{eqnarray}
\frac{p(\{\data_k\}\,|\,\ratepars)}{p(\{\data_k\}\,|\,\interim)} &=&
    \int
    \frac{p(\{\entry_k\}\,|\,\ratepars)}{p(\{\entry_k\}\,|\,\interim)}\,
    p(\{\entry_k\}\,|\,\{\data_k\},\,\interim)
    \dd\{\entry_k\} \quad.
\end{eqnarray}
The data only enter this equation through the posterior constraints provided
by the catalog!

A very simple example is a procedure that will be very familiar: the histogram.
If you model the function $p(\{\entry_k\}\,|\,\ratepars)$ as a piecewise
constant rate function---where the bin heights are the parameters---and if the
uncertainties on the catalog are negligible compared to the bin widths then
the maximum marginalized solution for \ratepars\ is a histogram of the
catalog entries (see \app{histogram} for the derivation).
The case of non-negligible uncertainties is described by \citet{hogge} using a
method similar to the one discussed below.
The limiting noiseless case for an incomplete catalog also has an analytic
maximum likelihood result that we derive in \app{histogram}.

\section{Model generalities}

In the inference procedure described above, we only discussed an abstract
occurence rate function.
To make it more concrete, we'll model the catalog as a Poisson draw from the
\emph{observable} rate function $\obs{\rate}_\ratepars$.
This leads to the well known result (see \citealt{tabachnik,youdin} for some
of the examples from the exoplanet literature)
\begin{eqnarray}\eqlabel{poisson-like}
p(\{\entry_k\}\,|\,\ratepars) &=&
    \exp\left(-\int \obs{\rate}_\ratepars (\entry) \dd\entry\right) \,
    \prod_{k=1}^K \obs{\rate}_\ratepars (\entry_k)\quad.
\end{eqnarray}
The main thing to note here is that $\obs{\rate}_\ratepars$ is the rate of
exoplanets that you would expect to observe taking into account the geometric
transit probability and any other detection efficiencies.
In practice, it probably makes sense to model the observable rate as
\begin{eqnarray}
\obs{\rate}_\ratepars(\entry) &=&
    \completeness(\entry)\,\rate_\ratepars(\entry)
\end{eqnarray}
where $\completeness(\entry)$ is the detection efficiency at \entry\ and
$\rate_\ratepars(\entry)$ is the object that we want to infer: the true
occurence rate.
Up to this point, we haven't discussed any specific functional form for
$\rate_\ratepars(\entry)$ and all of this derivation is equally applicable
whether we model the occurence rate as (for example) a broken power law or
a histogram.

For the results in this \paper, we will assume that the completeness function
$\completeness(\entry)$ is known empirically but that is not a requirement for
the validity of this method.
Instead, we could use a functional form for it and infer the parameters
(\dfmtodo{cite all the papers that do the ramp, step function, whatever}).

\paragraph{Inverse-detection-efficiency}
It's now interesting to take a brief aside and discuss the connection between
our model and the commonly used inverse-detection-efficiency procedure
(\citealt{howard,dressing,petigura}).
This procedure involves making a weighted histogram of the catalog entries
where the weight for object $\entry_k$ is $1/\completeness(\entry_k)$.
This makes intuitive sense but (to our knowledge) does not have a clear
probabilistic justification or interpretation.

If we model the occurence rate as a histogram with $J$ fixed bin volumes
$\binarea_j$
\begin{eqnarray}
\rate_\ratepars (\entry) &=& \left\{\begin{array}{ll}
\ratepar_1 & \entry \in \binarea_1,\\
\ratepar_2 & \entry \in \binarea_2,\\
\cdots \\
\ratepar_J & \entry \in \binarea_J,\\
0 & \mathrm{otherwise}
\end{array}\right.
\end{eqnarray}
then \eq{poisson-like} becomes
\begin{eqnarray}
\ln p(\{\entry_k\}\,|\,\ratepars) &=&
    \sum_{k=1}^K \sum_{j=1}^J \mathbf{1}[\entry_k \in
        \binarea_j]\,\ln\completeness(\entry_k)\,\ratepar_j
    -\sum_{j=1}^J\ratepar_j\,\int_{\binarea_j} \completeness(\entry)\dd\entry
\end{eqnarray}
where the indicator function $\mathbf{1}[\cdot]$ is one if $\cdot$ is true and
zero otherwise.
If we take the gradient of this function with respect to \ratepars\ and set it
equal to zero, we find the maximum likelihood result
\begin{eqnarray}
\ratepar_j &=& \frac{N_j}{\int_{\binarea_j} \completeness(\entry)\dd\entry}
\end{eqnarray}
where $N_j$ is the number of objects that fall within the bin $j$.
Since it is the maximum likelihood solution, this result is the minimum
variance estimator for the occurence rate in the limit of negligible catalog
uncertainties.
This is \emph{almost} the same as the inverse-detection-efficiency result and
in various limiting cases, they would provide the same result.
\dfmtodo{Finish explaining why this result is better under the specified
assumptions.}

\section{The completeness function \& dataset}

Erik Petigura and collaborators (\citealt{petigura-a,petigura}) heroically
determined the empirical end-to-end completeness of their transit search
pipeline by injecting synthetic signals of known period and radius into the
raw aperture photometry and testing recovery rates.
They also published a catalog of XXX small planet candidates around G type
stars; YYY of them previously unknown.
Given the unprecedented quality of the completeness characterization, we will
focus on this dataset in this \paper.

We use all the injected samples from \citet{petigura} to compute the mean
(marginalized) detection efficiency in bins of $\ln\period$ and $\ln\radius$.
In each bin, this efficiency is simply the fraction of recovered injections.
For the purposes of this \paper, we will neglect the uncertainties introduced
by the fact that this estimate is being made with a finite number of samples.

The largest injected radius is $16\,R_\oplus$ but, because of the measurement
uncertainties on the radii, it will be necessary for us to model the
distribution to larger radii.
To do this, we approximate the survey completeness for $\radius>16\,R_\oplus$
as 1.

\section{Validation using synthetic catalogs}

To confirm that the probabilistic framework developed in the previous sections
provides correct inferences about the population of exoplanets given noisy
data, we'll start by generating synthetic catalogs and running our full
inference pipeline on those data.
In this section, we also demonstrate the key benefits of our method over the
standard intuitive inverse-detection-efficiency procedure.

\paragraph{Inverse-detection-efficiency}
Authors in the exoplanet literature (for example, \citealt{howard, dressing,
petigura}) have built population level models using this procedure.
The basic idea is to make a histogram of planet candidates in the model where
each sample has a weight proportional to the inverse detection efficiency.
This method makes intuitive sense and, in the limit of infinitely precise
measurements, it is not wrong.
Even in this limit, this estimator of the occurrence distributions will have
large variance; especially as the detection efficiency becomes low.
In the presence of observational uncertainties, the inferred distributions
will be biased.

The likelihood function in \eq{true-like} provides a \emph{probability
distribution over catalogs} conditioned on the parameters of the rate
function.
Therefore, it is also the description of a \emph{generative process} for
synthetic catalogs.
To demonstrate the effectiveness of our model, we generate synthetic catalogs
from known distributions in period and radius, and test the performance of our
method on the resulting data.
For comparison, we also implement the one-over-efficiency weighting procedure
(see, for example, \citealt{howard, dressing, petigura}) and apply it to the
same datasets.

\paragraph{The generative process}
The variable rate Poisson likelihood function in \eq{true-like} can be sampled
generally for arbitrary forms of the rate function using a procedure called
``thinning'' (\citealt{poisson}).
In this \paper, however, we will only consider binned representations of
\rate, greatly simplifying the generative process.
Conditioned on a set of rate function parameters and a given completeness
function (here we'll use the injections from \citealt{petigura}), we compute
the observable rate grid from \eq{obs-rate}.
Then in each bin, we sample $J$ candidates uniformly in period and
radius (as defined by the bin edges) where $J$ is a Poisson random variable
sampled with the rate given by the observable rate in this bin.
\dfmtodo{Make this description not be the most confusing thing ever.}
Then, we add 30\% measurement uncertainties to the radius measurements where
30\% was chosen to match to properties of the real catalog
(\citealt{petigura}).

\paragraph{Results}
See \fig{synthetic-radius}.
As expected, the forward model is a much lower variance estimator.

\begin{figure}[htbp]
\begin{center}
\subfigure[one-over-efficiency]{%
\includegraphics[width=0.45\textwidth]{figures/synthetic-vmax-radius.pdf}
}
\subfigure[fully-marginalized forward model]{%
\includegraphics[width=0.45\textwidth]{figures/synthetic-radius.pdf}
}
\end{center}
\caption{%
The inferred distribution of exoplanet radii based on a noisy observation of a
synthetic catalog.
The red dotted line shows the true distribution function and the black
histograms are the constraints from the different inference techniques.
\figlabel{synthetic-radius}}
\end{figure}

\section{Real data}

It has been shown that the joint period--radius distribution for exoplanets is
not separable (\dfmtodo{CITE}).
In this section, we discuss non-separable model in our analysis framework.

We model the rate function $\rate(\period,\,\radius)$ as a piecewise constant
function in rectangular bins in $\ln\period$ and $\ln\radius$.
The parameters \ratepars\ that we're fitting are the values of each of these
bins (subject to a normalization constraint).

\section{Discussion}

We have made the first measurement of the Earth-like exoplanet population that
is based on forward modeling, takes proper account of the observational
uncertainties on planet radii, and makes use of extremely flexible
non-separable distributions for period and radius.
We find \hoggtodo{BLAH BLAH BLAH.}

Although our method makes weaker assumptions than any previous work in this
area, of course it \emph{does} make strong assumptions.
It assumes that the data set (from \citealt{petigura}) contains planets and
only planets (no false positives).
It assumes that the individual-planet radius uncertainties are also correctly
determined.
It assumes that the completeness calculation (again from \citealt{petigura})
is accurate and smoothly varying with period and radius; it makes no attempt
to adjust the completeness star-by-star for brightness or variability.
It assumes that the exoplanets are all independently drawn from the same
distribution function, with no regard to age, metallicity, or multiplicity.

Although we have taken account of the radius uncertainties in a principled
way, we did make one important approximation: We ignored planets with central
values far outside the period and radius box in which we built the model.
That is, we assumed that there are no very large outliers in radius
determination.
This assumption is a complex one; it involves the effective, end-to-end error
distribution in the data set, but also smoothness of the distribution function
at or near the edges of the analysis box.

If our inferences are right about true Earth analogs, then we expect that
there are about XXX transiting Earth analogs hidden in the \kepler\ data set.
We would be able to find some of these Earth analogs if the effective noise
level in the data could be reduced by a factor of about XXX.
We have identified a number of directions along which this noise reduction
might happen, including but not limited to retrospective data-driven or
physics-driven recalibrations of the \kepler\ focal plane
(\citealt{hoggwhitepaper}), new methods for performing aperture photometry,
and better models for stochastic instrument variability or stochastic
intrinsic stellar variability that take account of frequency structure
(\citealt{brewer, carter, roberts}).
If any (or all) of these deliver substantial improvements for \kepler\
archival data reanalyses, it's Nobel Prize time!  (Or at least \textsl{The
Colbert Report}?)

All of the code used in this project is available from
\url{http://github.com/dfm/exopop} under the MIT open-source software license.
This code (plus some dependencies) can be run to re-generate all of the
figures and results in this \paper; this version of the paper was generated
with git commit \texttt{\githash} (\gitdate).

\acknowledgments
It is a pleasure to thank
    Erik Petigura (Berkeley),
    \etc
for helpful contributions to the ideas and code presented here.
This project was partially supported by the NSF (grant AST-0908357), and NASA
(grant NNX08AJ48G).
This research made use of the NASA \project{Astrophysics Data System}.

\appendix

\section{Maximum likelihood estimators for noiseless catalogs}
\sectlabel{histogram}

Equation XXXX gives the likelihood of a catalog $\catalog =
\{\entry_k\}_{k=1}^K$, conditioned on a particular choice of occurence rate
parameters \ratepars:
\begin{eqnarray}
\ln p(\catalog\,|\,\ratepars) &=& \sum_{k=1}^K \ln \rate(\entry_k)
    - \int \rate(\entry) \dd\entry \quad.
\end{eqnarray}
If we model the occurence rate as a piecewise constant function
\begin{eqnarray}
\rate (\entry) &=& \left \{\begin{array}{ll}
\ratepar_1 & \mathrm{if\,} \entry \in \binarea_1, \\
\cdots & \cdots \\
\ratepar_N & \mathrm{if\,} \entry \in \binarea_N, \\
0 & \mathrm{otherwise}
\end{array}\right.
\end{eqnarray}
where $\binarea_n$ is the volume of the bin $n$, the log likelihood becomes
\begin{eqnarray}
\ln p(\catalog\,|\,\ratepars) &=&
    \sum_{k=1}^K \sum_{n=1}^N \mathbf{1}[\entry_k \in \binarea_n]
        \ln\ratepar_n
    - \sum_{n=1}^N \ratepar_n\,\binarea_n \quad.
\end{eqnarray}
The maximum likelihood solution to this can be computed analytically
\begin{eqnarray}
\frac{\dd \ln p(\catalog\,|\,\ratepars)}{\dd \ratepar_m} &=&
    \frac{N_m}{\ratepar_m} - \binarea_m \\
\to \quad \ratepar_m &=& \frac{N_m}{\binarea_m}
\end{eqnarray}
where $N_m$ is the number of objects in the bin $m$.
This result is simply a \emph{normalized histogram} of the points.

Including the completeness function $\completeness(\entry)$ is as simple as
modeling the observable occurence rate as
\begin{eqnarray}
\rate (\entry) &=& \completeness(\entry) \, \left \{\begin{array}{ll}
\ratepar_1 & \mathrm{if\,} \entry \in \binarea_1, \\
\cdots & \cdots \\
\ratepar_N & \mathrm{if\,} \entry \in \binarea_N, \\
0 & \mathrm{otherwise}
\end{array}\right. \quad.
\end{eqnarray}
This leads to a similar analytic maximum likelihood solution
\begin{eqnarray}
\frac{\dd \ln p(\catalog\,|\,\ratepars)}{\dd \ratepar_m} &=&
    \frac{N_m}{\ratepar_m}
    - \int_{\binarea_m} \completeness(\entry)\dd\entry \\
\to \quad \ratepar_m &=& N_m \,
    \left [\int_{\binarea_m} \completeness(\entry)\dd\entry \right ]^{-1}
\end{eqnarray}
where the remaining integral is over the bin $m$.
This result is \emph{almost} the same as the inverse-detection-efficiency
procedure but each point is weighted by the bin-averaged efficiency instead of
the per-object efficiency.

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\begin{thebibliography}{}\raggedright

\bibitem[Adams, Murray \& MacKay(2009)]{poiss-gp}
Adams, R.~P., Murray, I., \& MacKay, D.~J.~C.\ 2009, ICML, 2009, 9
(\href{http://homepages.inf.ed.ac.uk/imurray2/pub/09poisson/}{online})

\bibitem[Brewer \& Stello(2009)]{brewer}
Brewer, B.~J., \& Stello, D.\ 2009, \mnras, 395, 2226 (\arxiv{0902.3907})

\bibitem[Carter \& Winn(2009)]{carter}
Carter, J.~A., \& Winn, J.~N.\ 2009, \apj, 704, 51 (\arxiv{0909.0747})

\bibitem[Dong \& Zhu(2013)]{dong}
Dong, S., \& Zhu, Z.\ 2013, \apj, 778, 53 (\arxiv{1212.4853})

\bibitem[Dressing  \& Charbonneau(2013)]{dressing}
Dressing, C.~D., \& Charbonneau, D.\ 2013, \apj, 767, 95 (\arxiv{1302.1647})

\bibitem[Hogg \etal(2013)]{hoggwhitepaper}
Hogg, D.~W., Angus, R., Barclay, T., et al.\ 2013, \arxiv{1309.0653}

\bibitem[Hogg \etal(2010)]{hogge}
Hogg, D.~W., Myers, A.~D., \& Bovy, J.\ 2010, \apj, 725, 2166
(\arxiv{1008.4146})

\bibitem[Howard \etal(2012)]{howard}
Howard, A.~W., Marcy, G.~W., Bryson, S.~T., et al.\ 2012, \apjs, 201, 15
(\arxiv{1103.2541})

\bibitem[Lewis \& Shedler(1979)]{poisson}
Lewis, P.~A.~W., \& Shedler, G.~S.\ 1979, Naval Research Logistics Quarterly,
26, 403

\bibitem[Petigura \etal(2013a)]{petigura-a}
Petigura, E.~A., Marcy, G.~W., \& Howard, A.~W.\ 2013a, \apj, 770, 69
(\arxiv{1304.0460})

\bibitem[Petigura \etal(2013b)]{petigura}
Petigura, E.~A., Howard, A.~W., \& Marcy, G.~W.\ 2013b,
Proceedings of the National Academy of Science, 110, 19273 (\arxiv{1311.6806})

\bibitem[Roberts \etal(2013)]{roberts}
Roberts, S., McQuillan, A., Reece, S., \& Aigrain, S.\ 2013, \mnras, 435, 3639
(\arxiv{1308.3644})

\bibitem[Tabachnik \& Tremaine(2002)]{tabachnik}
Tabachnik, S., \& Tremaine, S.\ 2002, \mnras, 335, 151
(\arxiv{astro-ph/0107482})

\bibitem[Tremaine \& Dong(2012)]{tremaine}
Tremaine, S., \& Dong, S.\ 2012, \aj, 143, 94 (\arxiv{1106.5403})

\bibitem[Youdin(2011)]{youdin}
Youdin, A.~N.\ 2011, \apj, 742, 38 (\arxiv{1105.1782})

\end{thebibliography}

\end{document}
